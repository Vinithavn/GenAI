{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. RAG application using gemini model api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -qU langchain-google-genai\n",
    "# !pip install python-dotenv\n",
    "# !pip install -qU langchain-core\n",
    "# !pip install langchain-ollama "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import langchain\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"langchain_api_key\")\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the LLM model for the generation\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the embedding model for the vector similarity search\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Selecting the vector store - We will try with the in-memory vector store of langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "vector_store_1 = InMemoryVectorStore(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Document Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "bs4_strainer = bs4.SoupStrainer(class_=(\"post\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 Loading the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(web_path=\"https://jalammar.github.io/illustrated-transformer/\",\n",
    "                        bs_kwargs={\"parse_only\": bs4_strainer}\n",
    "                        )\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters: 24876\n",
      "\n",
      "The Illustrated Transformer\n",
      "\n",
      "Discussions:\n",
      "Hacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\n",
      "\n",
      "\n",
      "Translations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\n",
      "\n",
      "Watch: MITâ€™s Deep Learning State of the Art lecture referencing this post\n",
      "\n",
      "Featured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Update: This post has now become a book! Check \n"
     ]
    }
   ],
   "source": [
    "assert len(docs) == 1\n",
    "print(f\"Total characters: {len(docs[0].page_content)}\")\n",
    "print(docs[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 Split the document into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split blog post into 37 sub-documents.\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # chunk size (characters)\n",
    "    chunk_overlap=200,  # chunk overlap (characters)\n",
    "    add_start_index=True,  # track index in original document\n",
    ")\n",
    "split_chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"Split blog post into {len(split_chunks)} sub-documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.3 Store and index the document chunks in the vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['334a24e4-07f1-461d-b591-06b6a750cee7', 'b60fd671-c82a-4fa2-9840-6885fab59bf5', 'b39c62e1-521e-4342-baa9-236319045f40']\n"
     ]
    }
   ],
   "source": [
    "document_ids = vector_store_1.add_documents(documents=split_chunks)\n",
    "print(document_ids[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Preparing the rag prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "contextualize_q_system_prompt1 = \"\"\"Given a chat history and the latest user question \\\n",
    "which might reference context in the chat history, formulate a standalone question \\\n",
    "which can be understood without the chat history. Do NOT answer the question, \\\n",
    "just reformulate it if needed and otherwise return it as is.\"\"\"\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt1),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\n",
    "Use the following pieces of retrieved context to answer the question. \\\n",
    "If you don't know the answer, just say that you don't know. \\\n",
    "Use three sentences maximum and keep the answer concise.\\\n",
    "\n",
    "{context}\"\"\"\n",
    "\n",
    "contextualize_qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Preparing the RAG pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever,create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.messages import HumanMessage,AIMessage\n",
    "\n",
    "\n",
    "def retrieve_and_generate(question,chat_history):\n",
    "    retriever = vector_store_1.as_retriever()\n",
    "\n",
    "    #create a retriever which can include the chat history to retrieve the context\n",
    "    retriever_chain  = create_history_aware_retriever(\n",
    "        llm, retriever, contextualize_q_prompt\n",
    "        )\n",
    "    #LLM chain which passes the list of Documents from the retriever to the LLM\n",
    "    llm_answer_chain = create_stuff_documents_chain(llm,contextualize_qa_prompt)\n",
    "\n",
    "    #Final chain which gives the output context from the retriever chain to the LLM chain\n",
    "    retrieval_chain = create_retrieval_chain(retriever_chain, llm_answer_chain)\n",
    "\n",
    "    # Retriever chain has two inputs - user question and chat history. The context is automatically passed from the retriever throgh the create retriever chain\n",
    "    llm_response = retrieval_chain.invoke({\"input\": question, \"chat_history\": chat_history})\n",
    "\n",
    "    #Add the user question and LLM response to the chat history\n",
    "    chat_history.extend([HumanMessage(content=question), AIMessage(content=llm_response[\"answer\"])])\n",
    "    \n",
    "    return llm_response[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Self-attention allows a model to consider other parts of the input sequence when processing each word.  For each word, it creates query, key, and value vectors; the query vector's dot product with each key vector produces a score indicating relevance.  This weighting of other words helps create a richer encoding for the current word.\\n\""
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history = []\n",
    "question = \"Explain self attention?\"\n",
    "llm_response = retrieve_and_generate(question=question,chat_history = chat_history)\n",
    "llm_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Self-attention allows the model to weigh the importance of different words in a sentence when processing each word, leading to better understanding of context and relationships between words.  This is particularly useful for long sequences where RNNs might struggle to maintain context across many time steps.  Finally, it enables parallel processing, unlike recurrent models, speeding up training and inference.\\n'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"what is the advantages of it?\"\n",
    "llm_response = retrieve_and_generate(question=question,chat_history = chat_history)\n",
    "llm_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
